from "./rand_NUMBER" import rand_NUMBER;

// Struct of rewards list and selected-arms list
struct Records<N> {
    u32[N] rewards;
}

def main (u32 seed, u32 qu) -> Records<NUMBER> {

    // the random generator we need

    // linear congruential generator
    u32 a = 1314;  // Multiplier
    u32 c = 137;  // Increment
    u32 m = qu + 1;  // Modulus

    // Initalize the preference 

    // Theta matrix
    u32[3][2] mut theta = [[1,1],[1,1],[1,1]];
    u32[3][2] mut theta_temp = [[0,0],[0,0],[0,0]];

    // Probability vector
    u32[3] mut prob = [0,0,0];

    Records<NUMBER> mut r = Records {rewards: [0;NUMBER]};

    u32 mut reward = 0;
    u32 mut state = 0;
    u32 mut state_next = 0;

    // define the episode loop
    for u32 i in 0..NUMBER {

        // calculate the probability vector with the theta

        for u32 j in 0..3 {
            prob[j] = if theta[j][0] > theta[j][1] {
                prob[j] = qu * (theta[j][0] - theta[j][1]) / (theta[j][0] + 1);
            } else {
                prob[j] = qu * 2 / (theta[j][1] + 1);
            }
        }

        // generate NUMBER random numbers prepared for use
        u32[50] rand_nums = rand_50(seed, a, c, m);

        // in one episode, we have full 50 steps
        // we need to update the preference after the episode

        // set the reward to 0, and the state to 0
        reward = 50;
        state = 0;
        state_next = 0;

        for u32 k in 0..50 {
            // select the action with the probability vector
            state_next = if state == 0 {
                if rand_nums[k] < prob[0] {
                    0
                } else {
                    1
                }
            } else {
                if state == 1 {
                    if rand_nums[k] < prob[1] {
                        2
                    } else {
                        0
                    }
                } else {
                    if state == 2 {
                        if rand_nums[k] < prob[2] {
                            1
                        } else {
                            3
                        }
                    } else {
                        3
                    }
                }
            }

            // count into the theta_temp
            theta_temp[state][0] = if state == 0 {
                if state_next == 0 {
                    1
                } else {
                    0
                }
            } else {
                if state == 1 {
                    if state_next == 2 {
                        1
                    } else {
                        0
                    }
                } else {
                    if state == 2 {
                        if state_next == 1 {
                            1
                        } else {
                            0
                        }
                    } else {
                        0
                    }
                }
            }
            theta_temp[state][1] = if state == 0 {
                if state_next == 1 {
                    1
                } else {
                    0
                }
            } else {
                if state == 1 {
                    if state_next == 0 {
                        1
                    } else {
                        0
                    }
                } else {
                    if state == 2 {
                        if state_next == 3 {
                            1
                        } else {
                            0
                        }
                    } else {
                        0
                    }
                }
            }

            // get the reverse reward
            reward = if state_next == 3 {
                reward
            } else {
                reward - 1
            }
        }

        // update the theta matrix
        for u32 l in 0..3 {
            // update for the left move action in state l
            theta[l][0] = theta[l][0] - (theta_temp[l][0] * reward * prob[l] / qu) + (theta_temp[l][0] * reward);
            theta[l][1] = theta[l][1] + (theta_temp[l][0] * reward * prob[l] / qu) - (theta_temp[l][0] * reward);
            // update for the right move action in state l
            theta[l][0] = theta[l][0] - (theta_temp[l][1] * reward * prob[l] / qu)
            theta[l][1] = theta[l][1] + (theta_temp[l][1] * reward * prob[l] / qu)
        }

        theta_temp = [[0,0],[0,0],[0,0],[0,0]];
        r.rewards[i] = reward;
    }

    return r;
}